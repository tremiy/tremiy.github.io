

<!DOCTYPE html>
<html lang="en" color-mode=light>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>一些爬虫ing - tremiy</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />

  
  
  <meta name="description" content="一些爬虫
说明：一共爬了三个网站，分别是豆瓣TOP25..."> 
  
  <meta name="author" content="tremiy"> 

  
    <link rel="icon" href="/images/icons/favicon-16x16.png" type="image/png" sizes="16x16">
  
  
    <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="32x32">
  
  
    <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  
  
    <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
  
<link rel="stylesheet" href="//at.alicdn.com/t/font_1445822_onl0g0h21np.css">

  

  
  
  
<link rel="stylesheet" href="https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
  
  
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/xcode.min.css">

  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      lazyload: {
        enable: true,
        onlyPost: 'false',
        loading: '/images/theme/loading.gif'
      },
      donate: {
        enable: false,
        alipay: 'https://imgchr.com/i/BHcc6A',
        wechat: 'https://imgchr.com/i/BHctyR'
      },
      motto: {
        api: '',
        default: 'You can make it'
      },
      galleries: {
        enable: true
      },
      fab: {
        enable: true,
        alwaysShow: false
      },
      carrier: {
        enable: false
      },
      daovoice: {
        enable: true
      }
    }
  </script>

  

  
<meta name="generator" content="Hexo 4.2.1"></head>
<body class="lock-screen">
  <div class="loading"></div>
  


<nav class="navbar">
  <div class="left"></div>
  <div class="center">一些爬虫ing</div>
  <div class="right">
    
      <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
    
    <i class="iconfont iconmenu j-navbar-menu"></i>
  </div>
</nav>

  <nav class="menu">
  <div class="menu-wrap">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content">
      
      
      
      
      <li class="menu-item"><a href="/ " class="underline"> 首页</a></li>
      
      
      
      
      <li class="menu-item"><a href="/galleries " class="underline"> 课设</a></li>
      
      
      
      
      <li class="menu-item"><a href="/archives " class="underline"> 课外</a></li>
      
      
      
      
      <li class="menu-item"><a href="/tags " class="underline"> 工具</a></li>
      
      
      
      
      <li class="menu-item"><a href="/categories " class="underline"> 其他</a></li>
      
      
      
      
      <li class="menu-item"><a href="/about " class="underline"> 待定</a></li>
      
    </ul>
    <div class="menu-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
  </div>
</nav>
  <main id="main">
  <div class="container" id="container">
    <article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post-image.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">一些爬虫ing</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>January 10, 2022</span
        class="post-info-item">
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>16173</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        <h1 id="一些爬虫"><a href="#一些爬虫" class="headerlink" title="一些爬虫"></a>一些爬虫</h1><blockquote>
<p>说明：一共爬了三个网站，分别是豆瓣TOP250、51job、xssed网站漏洞信息，其中爬取豆瓣时使用的方法是BeautifulSoup，有可视化分析，后两个使用的方法是xpath，无可视化</p>
<p>软件：JetBrains PyCharm 2019.2.5</p>
<p>语言：python 3.8.5 </p>
</blockquote>
<p>在说明具体过程之前，先了解一些需要用到的python模块：</p>
<ul>
<li><code>import sys</code>：提供对解释器使用或维护的一些变量的访问，以及与解释器强烈交互的函数</li>
<li><code>from bs4 import BeautifulSoup</code>：页面解析，获取数据</li>
<li><code>import re</code>：正则表达式，进行文字匹配</li>
<li><code>import urllib.request</code>, <code>urllib.error</code>：指定URL，获取网页数据</li>
<li><code>import xlwt</code>：进行excel操作</li>
<li><code>import sqlite3</code>：进行SQLLi</li>
<li><code>import json</code> ：一种轻量级的数据交换格式</li>
<li><code>from lxml import etree</code>：用来解析XML字符串</li>
</ul>
<p><em>每次调用这些模块的时候都需要先引用，有一点抽象，等用到具体函数的时候再进一步解释</em></p>
<h2 id="爬取豆瓣TOP250"><a href="#爬取豆瓣TOP250" class="headerlink" title="爬取豆瓣TOP250"></a>爬取豆瓣TOP250</h2><p>爬取网页的过程分成三大步：</p>
<ol>
<li>爬取网页</li>
<li>解析数据</li>
<li>保存数据</li>
</ol>
<p>因此定义<code>main</code>函数为如下结构：</p>
<pre><code class="python">def main():
    baseurl = &quot;https://movie.douban.com/top250?start=&quot;
    # 1.爬取网页
    datalist = getData(baseurl)
    # 3.保存数据
    # savepath = &quot;.\\豆瓣电影TOP250.xls&quot;    # 使用excel保存爬取信息
    dbpath = &quot;movie.db&quot;
    # saveData(datalist,savepath)
    # askURL(baseurl)
    # saveData2DB(datalist,dbpath)

# 影片详情链接的规则
findLink = re.compile(r&#39;&lt;a href=&quot;(.*?)&quot;&gt;&#39;)  # 创建正则表达式对象，表示规则（字符串模式）
# 影片图片
findImgSrc = re.compile(r&#39;&lt;img.* scr=&quot;(.*?)&quot;&gt;&#39;, re.S)  # re.S：让换行符包括在字符中
# 影片片名
findTitle = re.compile(r&#39;&lt;span class=&quot;title&quot;&gt;(.*)&lt;/span&gt;&#39;)
# 影片评分
findRating = re.compile(r&#39;&lt;span class=&quot;rating_num&quot; property=&quot;v:average&quot;&gt;(.*)&lt;/span&gt;&#39;)
# 评价人数
findJudge = re.compile(r&#39;&lt;span&gt;(\d*)人评价&lt;/span&gt;&#39;)
# 找到概况
findInq = re.compile(r&#39;&lt;span class=&quot;inq&quot;&gt;(.*)&lt;/span&gt;&#39;)
# 找到影片的相关内容
findBd = re.compile(r&#39;&lt;p class=&quot;&quot;&gt;(.*?)&lt;/p&gt;&#39;, re.S)
</code></pre>
<h3 id="一、-爬取网页："><a href="#一、-爬取网页：" class="headerlink" title="一、 爬取网页："></a>一、 爬取网页：</h3><pre><code class="python">def getData(baseurl):
    datalist = []
    for i in range(0, 10):  # 调用获取页面信息的函数·10次
        url = baseurl + str(i * 25)
        html = askURL(url)  # 保存获取到的网页源码

        # 2.逐一解析数据
        soup = BeautifulSoup(html, &quot;html.parser&quot;)       #指定解析器
        for item in soup.find_all(&#39;div&#39;, class_=&quot;item&quot;):  # 类别的属性要加下划线，查找符合要求的字符串，形成列表
            # print(item)        #测试：查看电影item全部信息
            data = []  # 保存一部电影的所有信息
            item = str(item)

            # 影片详情的链接
            link = re.findall(findLink, item)[0]  # re库用来通过正则表达式查找指定的字符串
            data.append(link)

            imgSrc = re.findall(findImgSrc,item)
            data.append(imgSrc)  # 添加图片

            titles = re.findall(findTitle, item)  # 片名可能只有一个中文名，没有外国名
            if (len(titles) == 2):
                ctitle = titles[0]  # 添加中文名
                data.append(ctitle)
                otitle = titles[1].replace(&quot;/&quot;, &quot;&quot;)  # 去掉无关的符号
                data.append(otitle)  # 添加外国名
            else:
                data.append(titles[0])
                data.append(&#39; &#39;)  # 外国名留空

            rating = re.findall(findRating, item)[0]
            data.append(rating)  # 添加评分

            judgeNum = re.findall(findJudge, item)[0]
            data.append(judgeNum)  # 添加概述

            inq = re.findall(findInq, item)
            if len(inq) != 0:
                inq = inq[0].replace(&quot;。&quot;, &quot;&quot;)  # 去掉句号
                data.append(inq)  # 添加概述
            else:
                data.append(&quot;  &quot;)  # 留空

            bd = re.findall(findBd, item)[0]
            bd = re.sub(&#39;&lt;br(\s+)?/&gt;(\s+)?&#39;, &quot; &quot;, bd)  # 去掉&lt;br/&gt;
            bd = re.sub(&#39;/&#39;, &quot; &quot;, bd)  # 替换/
            data.append(bd.strip())  # 去掉前后的空格

            datalist.append(data)  # 把处理好的一部电影信息收入datalist
            print(datalist)


    print(datalist)

    return datalist</code></pre>
<pre><code class="python">
# 得到指定一个URL的网页内容
def askURL(url):
    head = {  # 模拟浏览器头部信息，向豆瓣服务器发送消息
        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36&quot;
    }
    # 用户代理，标识告诉豆瓣服务器，我们是什么类型的机器，浏览器（本质上是告诉浏览器，我们可以接受什么水平的内容）

    request = urllib.request.Request(url, headers=head)
    html = &quot;&quot;
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode(&quot;utf-8&quot;)
        # print(html)
    except urllib.error.URLError as e:
        if hasattr(e, &quot;code&quot;):      #如果对象e中有属性code则返回true
            print(e.code)
        if hasattr(e, &quot;reason&quot;):    #如果对象e中有属性reason则返回true
            print(e.reason)
    return html

</code></pre>
<pre><code class="python">
&#39;&#39;&#39;
# 3.保存数据
def saveData(savepath):
&#39;&#39;&#39;
def saveData(datalist,savepath):
    book = xlwt.Workbook(encoding=&quot;utf-8&quot;)  # 创建workbook对象
    sheet = book.add_sheet(&#39;豆瓣电影TOP250&#39;,cell_overwrite_ok=True)  # 创建工作表
    col = (&#39;电影详情链接&#39;,&quot;图片链接&quot;,&quot;影片中文名&quot;,&quot;影片外国名&quot;,&quot;评分&quot;,&quot;评份数&quot;,&quot;概况&quot;,&quot;相关信息&quot;)
    for i in range(0,8):
        sheet.write(0,i,col[i]) #列名
    for i in range(0,250):
        print(&quot;第%d条&quot;%i)
        data = datalist[i]
        for j in range(0,8):
            sheet.write(i+1,j,data[j])      #数据


    book.save(savepath)              # 保存数据表


def saveData2DB(datalist,dbpath):
    #init_db(dbpath)
    conn = sqlite3.connect(dbpath)  #建立数据库连接
    cur = conn.cursor()             #获取游标

    for data in datalist:
        for index in range(len(data)):
            if index == 4 or index == 5:
                continue
            data[index] = &#39;&quot;&#39;+str(data[index])+&#39;&quot;&#39;
        sql = &#39;&#39;&#39;
                insert into movie250(
                info_link,pic_link,cname,ename,score,rated,instroduction,info)
                values(%s);&#39;&#39;&#39;%&quot;,&quot;.join(&#39;%s&#39; %id for id in data) #data里的列表每一个都用逗号连接起来
        print(sql)
        cur.execute(sql)
        conn.commit()           #提交数据库操作
    cur.close()
    conn.close()                #关闭数据库连接


def init_db(dbpath):            # 创建数据库
    sql = &#39;&#39;&#39;
        create table movie250
        (
        id integer primary key autoincrement,
        info_link text,
        pic_link text,
        cname varchar ,
        ename varchar ,
        score numeric ,
        rated numeric ,
        instroduction text,
        info text
        );
    &#39;&#39;&#39;    #创建数据表
    conn = sqlite3.connect(dbpath)
    cursor = conn.cursor()
    cursor.execute(sql)     # 执行
    conn.commit()           # 事务提交，否则数据库得不到更新
    conn.close()

if __name__ == &quot;__main__&quot;:  # 当程序执行时
    # 调用函数
    main()
    # init_db(&quot;movietest.db&quot;)
    print(&quot;爬取完毕！&quot;)</code></pre>
<h4 id="可视化操作："><a href="#可视化操作：" class="headerlink" title="可视化操作："></a>可视化操作：</h4><pre><code class="python">from flask import Flask,render_template
import sqlite3

app = Flask(__name__)


@app.route(&#39;/&#39;)
def index():
    return render_template(&quot;index.html&quot;)


@app.route(&#39;/index&#39;)
def home():
    # return render_template(&quot;index.html&quot;)
    return index()

@app.route(&#39;/movie&#39;)
def movie():
    datalist = []
    con = sqlite3.connect(&quot;movie.db&quot;)
    cur = con.cursor()
    sql = &quot;select * from movie250&quot;
    data = cur.execute(sql)
    for item in data:
        datalist.append(item)
    cur.close()
    con.close()

    return render_template(&quot;movie.html&quot;,movies = datalist)

@app.route(&#39;/score&#39;)
def score():
    return render_template(&quot;score.html&quot;)

@app.route(&#39;/word&#39;)
def word():
    return render_template(&quot;word.html&quot;)

@app.route(&#39;/team&#39;)
def team():
    return render_template(&quot;team.html&quot;)




if __name__ == &#39;__main__&#39;:
    app.run()
</code></pre>
<h3 id="二、-爬取51job网站"><a href="#二、-爬取51job网站" class="headerlink" title="二、 爬取51job网站"></a>二、 爬取51job网站</h3><pre><code class="python">import sys
from bs4 import BeautifulSoup       #解析页面，获取数据
import re                           #正则表达式，进行文字匹配
import urllib.request,urllib.error  #指定URL，获取页面数据
import xlwt                         #进行excel操作
import sqlite3                      #数据库
import json
from lxml import etree

#1.爬取网页
#2.解析数据
#3.保存数据

def main():
    baseurl1 = &quot;https://search.51job.com/list/030200,000000,0000,00,9,99,%25E7%25BD%2591%25E7%25BB%259C%25E5%25AE%2589%25E5%2585%25A8,2,&quot;
    baseurl2 = &quot;.html&quot;
    dataList = getData(baseurl1,baseurl2)
    # savepath = &quot;.\\网安工作.xls&quot;
    # saveData(dataList,savepath)
    dbpath = &quot;wangan.db&quot;
    saveData2DB(dataList,dbpath)


#1.爬取网页
def getData(baseurl1,baseurl2):
    dataList = []
    for i in range(1,30):
        page = str(i)
        url = baseurl1 + page + baseurl2
        html = askURL(url)
        # print(html)
        newhtml = json.loads(html[0])       #化为字典
        # print(newhtml)
        for j in range(0,len(newhtml[&#39;engine_search_result&#39;])):
            data = []
            key = newhtml[&#39;engine_search_result&#39;][j]
            #职位名称
            job_name = key[&#39;job_name&#39;]
            data.append(job_name)

            # 职位链接
            job_href = key[&#39;job_href&#39;]
            data.append(job_href)
            # print(job_href)

            #关键词
            attribute_text = key[&#39;attribute_text&#39;]
            keyword = &#39;&#39;
            for m in attribute_text:
                keyword = keyword + &#39; &#39; + m
            data.append(keyword)

            # 工资待遇
            providesalary_text = key[&#39;providesalary_text&#39;]
            data.append(providesalary_text)

            # 工作福利
            jobwelf = key[&#39;jobwelf&#39;]
            data.append(jobwelf)

            jobhtml = askURL2(job_href)
            if jobhtml==0:
                continue
            # print(jobhtml)

            # 职位详情
            try:
                jobmessage = etree.HTML(jobhtml).xpath(&#39;/html/body/div[3]/div[2]/div[3]/div[1]/div//p/text()&#39;)
            except:
                continue
            # print(jobmessage)
            jobmessage1 = &#39;&#39;
            for k in jobmessage:
                jobmessage1 = jobmessage1 + k + &#39;\n&#39;
            jobmessage1 = jobmessage1.replace(&#39;\n&#39;,&#39;&#39;).replace(&#39;\xa0&#39;,&#39;&#39;).replace(&#39;\t&#39;,&#39;&#39;)
            if jobmessage1 == []:
                data.append(&#39;  &#39;)
            else:
                data.append(jobmessage1)

            # 工作地址
            jobadress = etree.HTML(jobhtml).xpath(&#39;/html/body/div[3]/div[2]/div[3]/div[2]/div//p/text()&#39;)
            # print(jobadress)
            if jobadress == []:
                data.append(&#39;  &#39;)
            else:
                data.append(jobadress[0])

            # 公司名称
            company_name = key[&#39;company_name&#39;]
            data.append(company_name)

            #公司类型
            companytype_text = key[&#39;companytype_text&#39;]
            data.append(companytype_text)



            #公司规模
            companysize_text = key[&#39;companysize_text&#39;]
            data.append(companysize_text)


            #公司链接
            company_href = key[&#39;company_href&#39;]
            data.append(company_href)


            #更新时间
            updatedate = key[&#39;updatedate&#39;]
            data.append(updatedate)

            dataList.append(data)
            print(&quot;爬取第%d页第%d条&quot; %(i,j))
            # print(dataList)
            # break
    return dataList


def askURL(url):
    head = {  # 模拟浏览器头部信息，向豆瓣服务器发送消息
        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36&quot;
    }
    request = urllib.request.Request(url,headers=head)
    html = &quot;&quot;
    newhtml = &quot;&quot;
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode(&quot;gbk&quot;)
        #print(html)
        tmp = re.compile(r&#39;window.__SEARCH_RESULT__ = (.*?)&lt;/script&gt;&#39;)
        newhtml = re.findall(tmp,html)
    except urllib.error.URLError as e:
        if hasattr(e,&quot;code&quot;):
            print(e.code)
        if hasattr(e,&quot;reason&quot;):
            print(e.reason)
    return newhtml

def askURL2(url):
    head = {  # 模拟浏览器头部信息，向豆瓣服务器发送消息
        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36&quot;
    }
    request = urllib.request.Request(url,headers=head)
    html = &quot;&quot;
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode(&quot;gbk&quot;)
    except:
        return 0
    return html


def saveData(datalist,savepath):
    book = xlwt.Workbook(encoding=&#39;utf-8&#39;)
    sheet = book.add_sheet(&#39;网安&#39;,cell_overwrite_ok=True)         #可重复对一个单元格进行操作
    col = (&quot;职位名称&quot;,&quot;职位链接&quot;,&quot;关键词&quot;,&quot;工资待遇&quot;,&quot;工作福利&quot;,&quot;职位详情&quot;,&quot;工作地址&quot;,&quot;公司名称&quot;,&quot;公司类型&quot;,&quot;公司规模&quot;,&quot;公司链接&quot;,&quot;更新时间&quot;)
    for i in range(0,12):
        sheet.write(0,i,col[i])
    for i in range(0,1440):
        print(&quot;保存第%d条&quot;%i)
        data = datalist[i]
        for j in range(0,12):
            sheet.write(i+1,j,data[j])

    book.save(savepath)

def saveData2DB(datalist,dbpath):
    init_db(dbpath)
    conn = sqlite3.connect(dbpath)      #连接数据库
    cur = conn.cursor()                 #建立数据库
    i = 1
    for data in datalist:
        sql=&#39;&#39;&#39;
                insert into wangan(
                job_name,job_href,attribute_text,providesalary_text,jobwelf,jobmessage,jobadress,company_name,companytype_text,companysize_text,company_href,updatedate)
                values(%s);&#39;&#39;&#39;%&quot;,&quot;.join(&#39;\&quot;%s\&quot;&#39; %id for id in data)
        print(&quot;保存第%d条数据&quot;%i)
        i=i+1
        print(sql)
        try:
            cur.execute(sql)
            conn.commit()
        except:
            continue
    cur.close()
    conn.close()



def init_db(dbpath):
    sql=&#39;&#39;&#39;
        create table wangan
        (
        id integer primary key autoincrement,
        job_name varchar,
        job_href text,
        attribute_text text, 
        providesalary_text varchar,
        jobwelf varchar,
        jobmessage varchar,
        jobadress text,
        company_name varchar,
        companytype_text varchar,
        companysize_text varchar,
        company_href text,
        updatedate text
        )
    &#39;&#39;&#39;
    conn = sqlite3.connect(dbpath)
    cur = conn.cursor()
    cur.execute(sql)
    conn.commit()
    cur.close()
    conn.close



if __name__ == &quot;__main__&quot;:
    #调用函数
    main()
    print(&quot;爬取完毕！&quot;)
</code></pre>
<h3 id="三、-爬取xssed网站中的漏洞信息"><a href="#三、-爬取xssed网站中的漏洞信息" class="headerlink" title="三、 爬取xssed网站中的漏洞信息"></a>三、 爬取xssed网站中的漏洞信息</h3><pre><code class="python">import sys
from bs4 import BeautifulSoup  # 页面解析，获取数据
import re  # 正则表达式，进行文字匹配
import urllib.request, urllib.error  # 指定URL，获取网页数据
import xlwt  # 进行excel操作
import sqlite3  # 进行SQLLi
import json
from lxml import etree



#爬取网页
#解析数据
#保存数据

def main():
    baseurl = &quot;http://www.xssed.com/archive/page=&quot;
    datalist = getData(baseurl)
    dbpath = &quot;BDAI.db&quot;
    savepath = &quot;.\\BDAI.xls&quot;
    saveData(datalist,savepath)


#爬取网页、解析数据
def getData(baseurl):
    datalist = []
    for i in range(1,31):
        url = baseurl + str(i)
        html = askUrl(url)
        # print(html)
        mirror = []
        print(&quot;爬取第%d页&quot;%i)
        for j in range(2,32):
            try:
                m = str(j)
                mirrorHref = etree.HTML(html).xpath(&#39;//*[@id=&quot;tableborder&quot;]/table/tr[&#39;+m+&#39;]/th[9]/a/@href&#39;)
                mirror.append(mirrorHref)
                # print(mirrorHref)
            except:
                continue
        # print(mirror)
        for k in range(0,30):
            # print(mirror[k][0])
            data = []
            newUrl = &quot;http://www.xssed.com&quot;+&quot;{}&quot;.format(mirror[k][0])
            newHtml = askUrl(newUrl)
            # print(newHtml)
            Datesubmitted = etree.HTML(newHtml).xpath(&#39;//*[@id=&quot;contentpaneOpen&quot;]/table/div[2]/tr/th[1]//text()&#39;)
            data.append(Datesubmitted[0])
            Datepublished = etree.HTML(newHtml).xpath(&#39;//*[@id=&quot;contentpaneOpen&quot;]/table/div[2]/tr/th[2]//text()&#39;)
            data.append(Datepublished[0])
            Author = etree.HTML(newHtml).xpath(&#39;//*[@id=&quot;contentpaneOpen&quot;]/table/div[3]/tr/th[1]/a//text()&#39;)
            data.append(Author[0])
            Domain = etree.HTML(newHtml).xpath(&#39;//*[@id=&quot;contentpaneOpen&quot;]/table/div[3]/tr/th[2]//text()&#39;)
            data.append(Domain[0])
            Pagerank = etree.HTML(newHtml).xpath(&#39;//*[@id=&quot;contentpaneOpen&quot;]/table/div[3]/tr/th[4]//text()&#39;)
            data.append(Pagerank[0])
            URL = etree.HTML(newHtml).xpath(&#39;//*[@id=&quot;contentpaneOpen&quot;]/table/div[4]/tr/th//text()&#39;)
            data.append(URL[0])
            # print(data)
            datalist.append(data)
        print(datalist)
    return datalist

def askUrl(url):
    head = {  # 模拟浏览器头部信息，向服务器发送消息
        &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36&quot;
    }
    request = urllib.request.Request(url,headers=head)
    html=&quot;&quot;
    try:
        response = urllib.request.urlopen(request)
        html = response.read().decode(&quot;iso-8859-1&quot;)
        # print(html)
    except urllib.error.URLError as e:
        if hasattr(e,&quot;code&quot;):
            print(e.code)
        if hasattr(e,&quot;reason&quot;):
            print(e.reaseon)
    return html

#保存数据
def saveData(datalist,savepath):
    book = xlwt.Workbook(encoding=&quot;utf-8&quot;)
    sheet = book.add_sheet(&#39;xss&#39;,cell_overwrite_ok=True)
    col = (&quot;Date submitted&quot;,&quot;Date published&quot;,&quot;Author&quot;,&quot;Domain&quot;,&quot;Pagerank&quot;,&quot;URL&quot;)
    for i in range(0,6):
        sheet.write(0,i,col[i])
    for i in range(0,900):
        print(&quot;保存第%d条&quot;%i)
        data = datalist[i]
        # print(data)
        for j in range(0,6):
            sheet.write(i+1,j,data[j])
    book.save(savepath)

if __name__==&quot;__main__&quot;:
    main()
    print(&quot;爬取完毕&quot;)</code></pre>

      </section>
      <section class="extra">
        
        
        
        
<nav class="nav">
  
    <a href="/2022/01/11/%E9%80%89%E8%AF%BE%E7%B3%BB%E7%BB%9F%EF%BC%88%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AE%9E%E9%AA%8C%EF%BC%89/"><i class="iconfont iconleft"></i>选课系统（数据库实验）</a>
  
  
    <a href="/2022/01/04/github%E4%BD%BF%E7%94%A8/">github使用<i class="iconfont iconright"></i></a>
  
</nav>

      </section>
      
      <section class="comments">
  
  <div class="btn" id="comments-btn">查看评论</div>
  
  
</section>
      
    </section>
  </div>
</article>
  </div>
</main>
  <footer class="footer">
  <div class="footer-social">
    
    
    
    
    
    <a href="tencent://message/?Menu=yes&uin=562602692 " target="_blank" onMouseOver="this.style.color= '#12B7F5'"
      onMouseOut="this.style.color='#ffffe0'">
      <i class="iconfont footer-social-item  iconQQ "></i>
    </a>
    
    
    
    
    
    <a href="javascript:; " target="_blank" onMouseOver="this.style.color= '#09BB07'"
      onMouseOut="this.style.color='#ffffe0'">
      <i class="iconfont footer-social-item  iconwechat-fill "></i>
    </a>
    
    
    
    
    
    <a href="https://www.instagram.com/izhaoo/ " target="_blank" onMouseOver="this.style.color= '#DA2E76'"
      onMouseOut="this.style.color='#ffffe0'">
      <i class="iconfont footer-social-item  iconinstagram "></i>
    </a>
    
    
    
    
    
    <a href="https://github.com/tremiy " target="_blank" onMouseOver="this.style.color= '#24292E'"
      onMouseOut="this.style.color='#ffffe0'">
      <i class="iconfont footer-social-item  icongithub-fill "></i>
    </a>
    
    
    
    
    
    <a href="MinLv_Y@163.com " target="_blank" onMouseOver="this.style.color='#ffffe0'"
      onMouseOut="this.style.color='#ffffe0'">
      <i class="iconfont footer-social-item  iconmail"></i>
    </a>
    
  </div>
  <div class="footer-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
</footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  <div class="fab fab-daovoice">
    <i class="iconfont iconcomment"></i>
  </div>
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
    
<script src="/js/color-mode.js"></script>

  
</body>


<script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>






<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>






<script src="https://cdn.bootcss.com/fancybox/3.5.7/jquery.fancybox.min.js"></script>






<script src="/js/utils.js"></script>
<script src="/js/modules.js"></script>
<script src="/js/zui.js"></script>
<script src="/js/script.js"></script>





<script>
  (function (i, s, o, g, r, a, m) {
    i["DaoVoiceObject"] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date();
    a = s.createElement(o), m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    a.charset = "utf-8";
    m.parentNode.insertBefore(a, m)
  })(window, document, "script", ('https:' == document.location.protocol ? 'https:' : 'http:') +
    "//widget.daovoice.io/widget/0f81ff2f.js", "daovoice")
  daovoice('init', {
    app_id: "abcdefg"
  }, {
    launcher: {
      disableLauncherIcon: true,
    },
  });
  daovoice('update');
</script>



<script>
  (function () {
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    } else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>


<script>
  var _hmt = _hmt || [];
  (function () {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?4c204d8bc027a0455b5fc642ac334ca8";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>










</html>